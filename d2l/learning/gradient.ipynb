{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601481212717",
   "display_name": "Python 3.7.6 64-bit ('DLENV': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(4, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=133, shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2 * tf.tensordot(x, x, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(TensorShape([4]),\n <tf.Tensor: id=133, shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>,\n <tf.Tensor: id=146, shape=(), dtype=float32, numpy=28.0>)"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "x.shape,x,f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=59, shape=(5,), dtype=float32, numpy=array([ 0.,  1.,  4.,  9., 16.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=array([0., 1., 2., 3., 4.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=167, shape=(), dtype=float32, numpy=28.0>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on class GradientTape in module tensorflow.python.eager.backprop:\n\nclass GradientTape(builtins.object)\n |  GradientTape(persistent=False, watch_accessed_variables=True)\n |  \n |  Record operations for automatic differentiation.\n |  \n |  Operations are recorded if they are executed within this context manager and\n |  at least one of their inputs is being \"watched\".\n |  \n |  Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\n |  where `trainable=True` is default in both cases) are automatically watched.\n |  Tensors can be manually watched by invoking the `watch` method on this context\n |  manager.\n |  \n |  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n |  be computed as:\n |  \n |  ```python\n |  x = tf.constant(3.0)\n |  with tf.GradientTape() as g:\n |    g.watch(x)\n |    y = x * x\n |  dy_dx = g.gradient(y, x) # Will compute to 6.0\n |  ```\n |  \n |  GradientTapes can be nested to compute higher-order derivatives. For example,\n |  \n |  ```python\n |  x = tf.constant(3.0)\n |  with tf.GradientTape() as g:\n |    g.watch(x)\n |    with tf.GradientTape() as gg:\n |      gg.watch(x)\n |      y = x * x\n |    dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n |  d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n |  ```\n |  \n |  By default, the resources held by a GradientTape are released as soon as\n |  GradientTape.gradient() method is called. To compute multiple gradients over\n |  the same computation, create a persistent gradient tape. This allows multiple\n |  calls to the gradient() method as resources are released when the tape object\n |  is garbage collected. For example:\n |  \n |  ```python\n |  x = tf.constant(3.0)\n |  with tf.GradientTape(persistent=True) as g:\n |    g.watch(x)\n |    y = x * x\n |    z = y * y\n |  dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n |  dy_dx = g.gradient(y, x)  # 6.0\n |  del g  # Drop the reference to the tape\n |  ```\n |  \n |  By default GradientTape will automatically watch any trainable variables that\n |  are accessed inside the context. If you want fine grained control over which\n |  variables are watched you can disable automatic tracking by passing\n |  `watch_accessed_variables=False` to the tape constructor:\n |  \n |  ```python\n |  with tf.GradientTape(watch_accessed_variables=False) as tape:\n |    tape.watch(variable_a)\n |    y = variable_a ** 2  # Gradients will be available for `variable_a`.\n |    z = variable_b ** 3  # No gradients will be available since `variable_b` is\n |                         # not being watched.\n |  ```\n |  \n |  Note that when using models you should ensure that your variables exist when\n |  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n |  first iteration not have any gradients:\n |  \n |  ```python\n |  a = tf.keras.layers.Dense(32)\n |  b = tf.keras.layers.Dense(32)\n |  \n |  with tf.GradientTape(watch_accessed_variables=False) as tape:\n |    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n |                             # `a.variables` will return an empty list and the\n |                             # tape will not be watching anything.\n |    result = b(a(inputs))\n |    tape.gradient(result, a.variables)  # The result of this computation will be\n |                                        # a list of `None`s since a's variables\n |                                        # are not being watched.\n |  ```\n |  \n |  Note that only tensors with real or complex dtypes are differentiable.\n |  \n |  Methods defined here:\n |  \n |  __del__(self)\n |  \n |  __enter__(self)\n |      Enters a context inside which operations are recorded on this tape.\n |  \n |  __exit__(self, typ, value, traceback)\n |      Exits the recording context, no further operations are traced.\n |  \n |  __init__(self, persistent=False, watch_accessed_variables=True)\n |      Creates a new GradientTape.\n |      \n |      Args:\n |        persistent: Boolean controlling whether a persistent gradient tape\n |          is created. False by default, which means at most one call can\n |          be made to the gradient() method on this object.\n |        watch_accessed_variables: Boolean controlling whether the tape will\n |          automatically `watch` any (trainable) variables accessed while the tape\n |          is active. Defaults to True meaning gradients can be requested from any\n |          result computed in the tape derived from reading a trainable `Variable`.\n |          If False users must explicitly `watch` any `Variable`s they want to\n |          request gradients from.\n |  \n |  batch_jacobian(self, target, source, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>, parallel_iterations=None, experimental_use_pfor=True)\n |      Computes and stacks per-example jacobians.\n |      \n |      See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant) for the\n |      definition of a Jacobian. This function is essentially an efficient\n |      implementation of the following:\n |      \n |      `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.\n |      \n |      Note that compared to `GradientTape.jacobian` which computes gradient of\n |      each output value w.r.t each input value, this function is useful when\n |      `target[i,...]` is independent of `source[j,...]` for `j != i`. This\n |      assumption allows more efficient computation as compared to\n |      `GradientTape.jacobian`. The output, as well as intermediate activations,\n |      are lower dimensional and avoid a bunch of redundant zeros which would\n |      result in the jacobian computation given the independence assumption.\n |      \n |      Example usage:\n |      \n |      ```python\n |      with tf.GradientTape() as g:\n |        x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n |        g.watch(x)\n |        y = x * x\n |      batch_jacobian = g.batch_jacobian(y, x)\n |      # batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n |      ```\n |      \n |      Args:\n |        target: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].\n |          `target[i,...]` should only depend on `source[i,...]`.\n |        source: A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].\n |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n |          alters the value which will be returned if the target and sources are\n |          unconnected. The possible values and effects are detailed in\n |          'UnconnectedGradients' and it defaults to 'none'.\n |        parallel_iterations: A knob to control how many iterations are dispatched\n |          in parallel. This knob can be used to control the total memory usage.\n |        experimental_use_pfor: If true, uses pfor for computing the Jacobian. Else\n |          uses a tf.while_loop.\n |      \n |      Returns:\n |        A tensor `t` with shape [b, y_1, ..., y_n, x1, ..., x_m] where `t[i, ...]`\n |        is the jacobian of `target[i, ...]` w.r.t. `source[i, ...]`, i.e. stacked\n |        per-example jacobians.\n |      \n |      Raises:\n |        RuntimeError: If called on a non-persistent tape with eager execution\n |          enabled and without enabling experimental_use_pfor.\n |        ValueError: If vectorization of jacobian computation fails or if first\n |          dimension of `target` and `source` do not match.\n |  \n |  gradient(self, target, sources, output_gradients=None, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>)\n |      Computes the gradient using operations recorded in context of this tape.\n |      \n |      Args:\n |        target: Tensor (or list of tensors) to be differentiated.\n |        sources: a list or nested structure of Tensors or Variables. `target`\n |          will be differentiated against elements in `sources`.\n |        output_gradients: a list of gradients, one for each element of\n |          target. Defaults to None.\n |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n |          alters the value which will be returned if the target and sources are\n |          unconnected. The possible values and effects are detailed in\n |          'UnconnectedGradients' and it defaults to 'none'.\n |      \n |      Returns:\n |        a list or nested structure of Tensors (or IndexedSlices, or None),\n |        one for each element in `sources`. Returned structure is the same as\n |        the structure of `sources`.\n |      \n |      Raises:\n |        RuntimeError: if called inside the context of the tape, or if called more\n |         than once on a non-persistent tape.\n |        ValueError: if the target is a variable or if unconnected gradients is\n |         called with an unknown value.\n |  \n |  jacobian(self, target, sources, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>, parallel_iterations=None, experimental_use_pfor=True)\n |      Computes the jacobian using operations recorded in context of this tape.\n |      \n |      See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant) for the\n |      definition of a Jacobian.\n |      \n |      Example usage:\n |      \n |      ```python\n |      with tf.GradientTape() as g:\n |        x  = tf.constant([1.0, 2.0])\n |        g.watch(x)\n |        y = x * x\n |      jacobian = g.jacobian(y, x)\n |      # jacobian value is [[2., 0.], [0., 4.]]\n |      ```\n |      \n |      Args:\n |        target: Tensor to be differentiated.\n |        sources: a list or nested structure of Tensors or Variables. `target`\n |          will be differentiated against elements in `sources`.\n |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n |          alters the value which will be returned if the target and sources are\n |          unconnected. The possible values and effects are detailed in\n |          'UnconnectedGradients' and it defaults to 'none'.\n |        parallel_iterations: A knob to control how many iterations are dispatched\n |          in parallel. This knob can be used to control the total memory usage.\n |        experimental_use_pfor: If true, vectorizes the jacobian computation. Else\n |          falls back to a sequential while_loop. Vectorization can sometimes fail\n |          or lead to excessive memory usage. This option can be used to disable\n |          vectorization in such cases.\n |      \n |      Returns:\n |        A list or nested structure of Tensors (or None), one for each element in\n |        `sources`. Returned structure is the same as the structure of `sources`.\n |        Note if any gradient is sparse (IndexedSlices), jacobian function\n |        currently makes it dense and returns a Tensor instead. This may change in\n |        the future.\n |      \n |      \n |      Raises:\n |        RuntimeError: If called on a non-persistent tape with eager execution\n |          enabled and without enabling experimental_use_pfor.\n |        ValueError: If vectorization of jacobian computation fails.\n |  \n |  reset(self)\n |      Clears all information stored in this tape.\n |      \n |      Equivalent to exiting and reentering the tape context manager with a new\n |      tape. For example, the two following code blocks are equivalent:\n |      \n |      ```\n |      with tf.GradientTape() as t:\n |        loss = loss_fn()\n |      with tf.GradientTape() as t:\n |        loss += other_loss_fn()\n |      t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n |      \n |      \n |      # The following is equivalent to the above\n |      with tf.GradientTape() as t:\n |        loss = loss_fn()\n |        t.reset()\n |        loss += other_loss_fn()\n |      t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n |      ```\n |      \n |      This is useful if you don't want to exit the context manager for the tape,\n |      or can't because the desired reset point is inside a control flow construct:\n |      \n |      ```\n |      with tf.GradientTape() as t:\n |        loss = ...\n |        if loss > k:\n |          t.reset()\n |      ```\n |  \n |  stop_recording(self)\n |      Temporarily stops recording operations on this tape.\n |      \n |      Operations executed while this context manager is active will not be\n |      recorded on the tape. This is useful for reducing the memory used by tracing\n |      all computations.\n |      \n |      For example:\n |      \n |      ```\n |        with tf.GradientTape(persistent=True) as t:\n |          loss = compute_loss(model)\n |          with t.stop_recording():\n |            # The gradient computation below is not traced, saving memory.\n |            grads = t.gradient(loss, model.variables)\n |      ```\n |      \n |      Yields:\n |        None\n |      Raises:\n |        RuntimeError: if the tape is not currently recording.\n |  \n |  watch(self, tensor)\n |      Ensures that `tensor` is being traced by this tape.\n |      \n |      Args:\n |        tensor: a Tensor or list of Tensors.\n |      \n |      Raises:\n |        ValueError: if it encounters something that is not a tensor.\n |  \n |  watched_variables(self)\n |      Returns variables watched by this tape in order of construction.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
    }
   ],
   "source": [
    "help(tf.GradientTape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=167, shape=(), dtype=float32, numpy=28.0>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=183, shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "x_grad = t.gradient(y, x)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=82, shape=(), dtype=float32, numpy=60.0>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=187, shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "x_grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "x.assign(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=229, shape=(4,), dtype=float32, numpy=array([ 4.,  8., 12., 16.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "y\n",
    "x_grad = t.gradient(y, x)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=235, shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "x_grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=247, shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = tf.reduce_sum(x)\n",
    "t.gradient(y, x)  # Overwritten by the newly calculated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=242, shape=(), dtype=float32, numpy=10.0>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>,\n <tf.Tensor: id=256, shape=(4,), dtype=float32, numpy=array([ 1.,  4.,  9., 16.], dtype=float32)>,\n <tf.Tensor: id=262, shape=(4,), dtype=float32, numpy=array([2., 4., 6., 8.], dtype=float32)>)"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = x * x\n",
    "x,y,t.gradient(y, x)  # Same as `y = tf.reduce_sum(x * x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=704, shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "x = tf.range(4, dtype=tf.float32)\n",
    "x = tf.Variable(x)\n",
    "with tf.GradientTape() as t1:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "     \n",
    "\n",
    "grad_y_x = t1.gradient(y, x)\n",
    "grad_y_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=750, shape=(), dtype=float32, numpy=1.0>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "y= tf.Variable(y)\n",
    "with tf.GradientTape() as t2:\n",
    "    z = tf.reduce_sum(y)\n",
    "\n",
    "grad_z_y = t2.gradient(z, y) \n",
    "grad_z_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Tensor: id=787, shape=(), dtype=float32, numpy=3.0>,\n <tf.Tensor: id=788, shape=(), dtype=float32, numpy=2.0>)"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "# multi variables\n",
    "a = tf.Variable(2.0)\n",
    "b = tf.Variable(3.0)\n",
    "with tf.GradientTape() as t2:\n",
    "    c = a * b\n",
    "\n",
    "grad_c_a_b = t2.gradient(c, (a, b)) \n",
    "grad_c_a_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: id=808, shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "# Set `persistent=True` to run `t.gradient` more than once\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x * x\n",
    "    u = tf.stop_gradient(y)\n",
    "    z = u * x\n",
    "\n",
    "x_grad = t.gradient(z, x)\n",
    "x_grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Tensor: id=867, shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)>,\n <tf.Tensor: id=861, shape=(4,), dtype=float32, numpy=array([ 0.,  3., 12., 27.], dtype=float32)>)"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "# Set `persistent=True` to run `t.gradient` more than once\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x * x\n",
    "    #u = tf.stop_gradient(y)\n",
    "    z = y * x\n",
    "\n",
    "z_x_grad = t.gradient(z, x)\n",
    "y_x_grad = t.gradient(y, x)\n",
    "y_x_grad, z_x_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.5346855>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "a = tf.Variable(tf.random.normal(shape=()))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.range(10, dtype=tf.float32)\n",
    "a = tf.Variable(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}